{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rF2x3qooyBTI"
   },
   "source": [
    "# Generative adversarial network for financial time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "#!pip install nomkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (2.9.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.23.2)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.47.0)\n",
      "Requirement already satisfied: setuptools in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (58.1.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: packaging in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (4.3.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.10.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (2.9.1)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: packaging in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.47.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (4.3.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.23.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: setuptools in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (58.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.10.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (22.0.4)\n",
      "Collecting pip\n",
      "  Using cached pip-22.2.2-py3-none-any.whl (2.0 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.0.4\n",
      "    Uninstalling pip-22.0.4:\n",
      "      Successfully uninstalled pip-22.0.4\n",
      "Successfully installed pip-22.2.2\n",
      "Requirement already satisfied: tensorflow in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (2.9.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (4.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.23.2)\n",
      "Requirement already satisfied: setuptools in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (58.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.47.0)\n",
      "Requirement already satisfied: packaging in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.10.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/hyunjimoon/GoogleDrive_hmb/tolzul/DataInDM/Code_Commit/4_Auto/GeomPrior/venv/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Requires the latest pip\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Current stable release for CPU and GPU\n",
    "!pip install tensorflow\n",
    "\n",
    "# Or try the preview build (unstable)\n",
    "#pip install tf-nightly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mConfigProto()\n\u001b[1;32m      3\u001b[0m config\u001b[38;5;241m.\u001b[39mgpu_options\u001b[38;5;241m.\u001b[39mallow_growth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_addons\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfa\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tf\u001b[38;5;241m.\u001b[39m__version__\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "YfIk2es3hJEd",
    "outputId": "9807249a-c4bf-4b55-a00f-05e37446e5a8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "#import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "## Create the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMkJ4tjkWV4I"
   },
   "outputs": [],
   "source": [
    "# residual link used in attention\n",
    "class ResidualLink(layers.Layer):\n",
    "    def __init__(self):\n",
    "      super(ResidualLink, self).__init__()\n",
    "    def build(self, input_shape):  \n",
    "      self.sigma = tf.Variable(0.0, trainable=True)\n",
    "    def call(self, x, attn):  \n",
    "      return x + self.sigma * attn\n",
    "\n",
    "def trianglular_mask(long,short,shrink,dim):\n",
    "    mask = np.zeros([long, short], dtype=bool)\n",
    "    for i in range(long):\n",
    "      mask[i,max(0,int(i/long*short)-shrink//2):int(i/long*short)+1] = True\n",
    "    if dim ==3:\n",
    "      return mask[np.newaxis,shrink:]\n",
    "    elif dim ==4:\n",
    "      return mask[np.newaxis,np.newaxis,shrink:]\n",
    "    else:\n",
    "      raise ValueError\n",
    "\n",
    "# multi-head attention block, supports sparse attention\n",
    "def multi_head_attn_block(x, hidden_ratio, g_ratio, nH, sparse, GD, shrink=0, **kwargs):\n",
    "    def sparse_mask(long, short, kind):\n",
    "        '''\n",
    "        mask for sparse attention, \n",
    "        kind from LeftFloorMask, RightFloorMask, LeftRepetitiveMask and RightRepetitiveMask, \n",
    "        used in Your Local GAN\n",
    "        '''\n",
    "        stride = int(np.sqrt(short))\n",
    "        assert long % short == 0\n",
    "        multiple = long//short\n",
    "        if kind in ['LeftFloorMask', 'RightFloorMask']:\n",
    "            indices = []\n",
    "            for row in range(short):\n",
    "                for col in range(row - (row % stride), row + 1):\n",
    "                    indices.append([row, col])\n",
    "            indices = np.array(indices)\n",
    "            mask = np.zeros([short, short], dtype=bool)\n",
    "            if kind == 'LeftFloorMask':          \n",
    "                mask[indices[:, 0], indices[:, 1]] = True \n",
    "            else:   \n",
    "                mask[indices[:, 1], indices[:, 0]] = True \n",
    "\n",
    "        if kind in ['LeftRepetitiveMask', 'RightRepetitiveMask']:\n",
    "            if kind == 'RightRepetitiveMask':\n",
    "                col_indices = np.arange(0,short,stride)\n",
    "            else:\n",
    "                col_indices = np.arange(stride - 1,short,stride)\n",
    "            mask = np.eye(short, dtype=bool)\n",
    "            for col in col_indices:\n",
    "                mask[:,col] = True\n",
    "        return np.vstack([mask]*multiple)\n",
    "\n",
    "    def get_grid_masks(long, short, nH):\n",
    "        return np.repeat(np.array([sparse_mask(long,short,'RightFloorMask'),\n",
    "        sparse_mask(long,short,'LeftFloorMask'),\n",
    "        sparse_mask(long,short,'RightRepetitiveMask'),\n",
    "        sparse_mask(long,short,'LeftRepetitiveMask')]), nH//4, axis=0)\n",
    "\n",
    "    _, location_num, num_channels = x.shape.as_list()\n",
    "    assert num_channels % (hidden_ratio*nH) == 0       \n",
    "    downsampled_num = location_num // 2\n",
    "    hidden_size = num_channels // hidden_ratio             \n",
    "    head_size = hidden_size // nH         \n",
    "\n",
    "    # theta path\n",
    "    if GD == 'G':\n",
    "      theta = tfa.layers.SpectralNormalization(layers.Conv1D(hidden_size, 1, strides=1, padding='same', **kwargs))(x[:,shrink:])\n",
    "      theta = tf.reshape(theta, [-1, location_num-shrink, nH, head_size])\n",
    "      theta = tf.transpose(theta, [0, 2, 1, 3])\n",
    "    else:\n",
    "      theta = tfa.layers.SpectralNormalization(layers.Conv1D(hidden_size, 1, strides=1, padding='same', **kwargs))(x)\n",
    "      theta = tf.reshape(theta, [-1, location_num, nH, head_size])\n",
    "      theta = tf.transpose(theta, [0, 2, 1, 3])\n",
    "\n",
    "    # phi path\n",
    "    phi = tfa.layers.SpectralNormalization(layers.Conv1D(hidden_size, 1, strides=1, padding='same', **kwargs))(x)\n",
    "    phi = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding=\"valid\")(phi)\n",
    "    phi = tf.reshape(phi, [-1, downsampled_num, nH, head_size])       \n",
    "    phi = tf.transpose(phi, [0, 2, 1, 3])\n",
    "\n",
    "    attn = tf.matmul(theta, phi, transpose_b=True) #/ tf.math.sqrt(tf.cast(theta.shape[-1], tf.float32))\n",
    "    if sparse:\n",
    "      masks = tf.constant(get_grid_masks(location_num, downsampled_num, nH)) # acquire masks\n",
    "      attn = tf.keras.layers.Softmax()(attn, masks)\n",
    "    else:\n",
    "      if GD == 'G':\n",
    "        masks = tf.constant(trianglular_mask(location_num, downsampled_num, shrink, dim=4)) # acquire masks\n",
    "        attn = tf.keras.layers.Softmax()(attn, masks)\n",
    "        attn = attn\n",
    "      else: \n",
    "        attn = tf.keras.layers.Softmax()(attn)\n",
    "\n",
    "    # g path\n",
    "    g_hidden = num_channels // g_ratio\n",
    "    g_head_size = g_hidden // nH\n",
    "    g = tfa.layers.SpectralNormalization(layers.Conv1D(g_hidden, 1, strides=1, padding='same', **kwargs))(x)\n",
    "    g = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding=\"valid\")(g)\n",
    "    g = tf.reshape(g, [-1, downsampled_num, nH, g_head_size])\n",
    "    # swap for heads\n",
    "    g = tf.transpose(g, [0, 2, 1, 3])\n",
    "\n",
    "    attn_g = tf.matmul(attn, g)\n",
    "    # put heads to the end\n",
    "    attn_g = tf.transpose(attn_g, [0, 2, 3, 1])\n",
    "    if GD == 'G':\n",
    "      attn_g = tf.reshape(attn_g, [-1, location_num-shrink, g_hidden])\n",
    "    else:\n",
    "      attn_g = tf.reshape(attn_g, [-1, location_num, g_hidden])\n",
    "    attn_g = tfa.layers.SpectralNormalization(layers.Conv1D(num_channels, 1, strides=1, padding='same', **kwargs))(attn_g)\n",
    "    if GD == 'G': x = x[:,shrink:]\n",
    "    return ResidualLink()(x, attn_g)\n",
    "\n",
    "# ResNet generator block, used in SAGAN, etc.\n",
    "def G_res_block(x, out_channels, g_kernel, act=layers.ReLU, **kwargs):\n",
    "    x_0 = x\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = act()(x) \n",
    "    x = tfa.layers.SpectralNormalization(layers.Conv1D(out_channels, g_kernel, strides=1, padding='valid', **kwargs))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = act()(x) \n",
    "    x = tfa.layers.SpectralNormalization(layers.Conv1D(out_channels, g_kernel, strides=1, padding='valid', **kwargs))(x)    \n",
    "    x_0 = tfa.layers.SpectralNormalization(layers.Conv1D(out_channels, 1, strides=1, padding='same', **kwargs))(x_0)  \n",
    "    return x_0[:,-x.shape[1]:] + x\n",
    "\n",
    "# ResNet generator output layer\n",
    "def G_out_sn_block(x, out_channels, g_kernel, act=layers.ReLU, **kwargs):\n",
    "    x = act()(x) \n",
    "    x = tfa.layers.SpectralNormalization(layers.Conv1D(x.shape[-1], 1, strides=1, padding='same', **kwargs))(x)\n",
    "    x = act()(x) \n",
    "    x = tfa.layers.SpectralNormalization(layers.Conv1D(out_channels, 1, strides=1, padding='same', **kwargs))(x)\n",
    "    return x\n",
    "\n",
    "# ResNet discrinimtor start block\n",
    "def D_res_start_block(x, out_channels, d_kernel, bn=True, act=layers.LeakyReLU, **kwargs):\n",
    "    x_0 = x\n",
    "    x = tfa.layers.SpectralNormalization(layers.Conv1D(out_channels, d_kernel, strides=1, padding='same', **kwargs))(x)\n",
    "    x = act()(x) \n",
    "    x = tfa.layers.SpectralNormalization(layers.Conv1D(out_channels, d_kernel, strides=2, padding='same', **kwargs))(x)\n",
    "    x_0 = tfa.layers.SpectralNormalization(layers.Conv1D(out_channels, 2, strides=2, padding='same', **kwargs))(x_0)  \n",
    "    return x + x_0\n",
    "\n",
    "# ResNet discrinimtor block    \n",
    "def D_res_block(x, out_channels, d_kernel, bn=True, act=layers.LeakyReLU, **kwargs):\n",
    "    x_0 = x\n",
    "    if bn: x = layers.BatchNormalization()(x)\n",
    "    x = act()(x)\n",
    "    x = tfa.layers.SpectralNormalization(layers.Conv1D(out_channels, d_kernel, strides=1, padding='same', **kwargs))(x)\n",
    "    if bn: x = layers.BatchNormalization()(x)\n",
    "    x = act()(x)\n",
    "    x = tfa.layers.SpectralNormalization(layers.Conv1D(out_channels, d_kernel, strides=2, padding='same', **kwargs))(x)\n",
    "    x_0 = tfa.layers.SpectralNormalization(layers.Conv1D(out_channels, 2, strides=2, padding='same', **kwargs))(x_0)\n",
    "    return x_0 + x\n",
    "\n",
    "# ResNet discrinimtor last block\n",
    "def D_res_keep_block(x, d_kernel, bn=True, act=layers.LeakyReLU, **kwargs):\n",
    "    input_channels = x.shape.as_list()[-1]\n",
    "    x_0 = x\n",
    "    if bn: x = layers.BatchNormalization()(x)\n",
    "    x = act()(x)\n",
    "    x = tfa.layers.SpectralNormalization(layers.Conv1D(input_channels, d_kernel, strides=1, padding='same', **kwargs))(x)\n",
    "    if bn: x = layers.BatchNormalization()(x)\n",
    "    x = act()(x)\n",
    "    x = tfa.layers.SpectralNormalization(layers.Conv1D(input_channels, d_kernel, strides=1, padding='same', **kwargs))(x)\n",
    "    return x_0 + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tEyxE-GMC48"
   },
   "source": [
    "### The Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bpTcDqoLWjY"
   },
   "outputs": [],
   "source": [
    "def make_generator_model(data_len, data_channel, noise_len, noise_channel, gf_dim, choice, attn, act=layers.ReLU, g_kernel=5):\n",
    "    if choice == 'sagan':\n",
    "        attn_shrink = (noise_len - data_len - 6*2*(g_kernel-1))\n",
    "        inputs = layers.Input(shape= (noise_len, noise_channel))\n",
    "        x = tfa.layers.SpectralNormalization(layers.Conv1D(gf_dim, 1, strides=1, padding='same'))(inputs)\n",
    "        skip = tfa.layers.SpectralNormalization(layers.Conv1D(gf_dim, 1, strides=1, padding='same'))(x[:,-data_len:])\n",
    "        x = G_res_block(x, gf_dim, g_kernel, act=act) \n",
    "        skip += tfa.layers.SpectralNormalization(layers.Conv1D(gf_dim, 1, strides=1, padding='same'))(x[:,-data_len:])\n",
    "        x = G_res_block(x, gf_dim, g_kernel, act=act) \n",
    "        skip += tfa.layers.SpectralNormalization(layers.Conv1D(gf_dim, 1, strides=1, padding='same'))(x[:,-data_len:])\n",
    "        x = G_res_block(x, gf_dim, g_kernel, act=act) \n",
    "        skip += tfa.layers.SpectralNormalization(layers.Conv1D(gf_dim, 1, strides=1, padding='same'))(x[:,-data_len:])\n",
    "        x = multi_head_attn_block(x, hidden_ratio=attn[0], g_ratio=attn[1], nH=attn[2], sparse=attn[3], GD='G', shrink=attn_shrink) \n",
    "        skip += tfa.layers.SpectralNormalization(layers.Conv1D(gf_dim, 1, strides=1, padding='same'))(x[:,-data_len:])\n",
    "        x = G_res_block(x, gf_dim, g_kernel, act=act) \n",
    "        skip += tfa.layers.SpectralNormalization(layers.Conv1D(gf_dim, 1, strides=1, padding='same'))(x[:,-data_len:])\n",
    "        x = G_res_block(x, gf_dim, g_kernel, act=act)   \n",
    "        skip += tfa.layers.SpectralNormalization(layers.Conv1D(gf_dim, 1, strides=1, padding='same'))(x[:,-data_len:])\n",
    "        x = G_res_block(x, gf_dim, g_kernel, act=act)    \n",
    "        skip += tfa.layers.SpectralNormalization(layers.Conv1D(gf_dim, 1, strides=1, padding='same'))(x)\n",
    "        y = G_out_sn_block(skip, data_channel, g_kernel, act=act) \n",
    "        model = tf.keras.Model(inputs, y)\n",
    "\n",
    "    elif choice == 'transgan':\n",
    "        def mlp_block(x, ratio=4):\n",
    "          x0 = x\n",
    "          num_channels = x.shape[-1]\n",
    "          x = tfa.layers.SpectralNormalization(layers.Conv1D(num_channels * ratio, 1, strides=1, padding='same'))(x)\n",
    "          x = act()(x)\n",
    "          x = tfa.layers.SpectralNormalization(layers.Conv1D(num_channels, 1, strides=1, padding='same'))(x)\n",
    "          return x0 + x\n",
    "        def block(x, shrink):\n",
    "          x = layers.BatchNormalization()(x)\n",
    "          x = multi_head_attn_block(x, hidden_ratio=attn[0], g_ratio=attn[1], nH=attn[2], sparse=attn[3], GD='G', shrink=shrink)\n",
    "          x = layers.BatchNormalization()(x)\n",
    "          x = mlp_block(x)                     \n",
    "          return x\n",
    "        def end_block(x, num_channels):               \n",
    "          x = act()(x)\n",
    "          x = tfa.layers.SpectralNormalization(layers.Conv1D(x.shape[-1], 1, strides=1, padding='same'))(x)\n",
    "          x = act()(x)\n",
    "          x = tfa.layers.SpectralNormalization(layers.Conv1D(num_channels, 1, strides=1, padding='same'))(x)\n",
    "          return x   \n",
    "        inputs = layers.Input(shape= (noise_len, noise_channel))\n",
    "        x = tfa.layers.SpectralNormalization(layers.Conv1D(gf_dim, 1, strides=1, padding='same'))(inputs)               \n",
    "        skip = tfa.layers.SpectralNormalization(layers.Conv1D(gf_dim, 1, strides=1, padding='same'))(x[:,-data_len:])\n",
    "        x = block(x,64 + noise_len - data_len-126)  \n",
    "        skip = skip + tfa.layers.SpectralNormalization(layers.Conv1D(gf_dim, 1, strides=1, padding='same'))(x[:,-data_len:])\n",
    "        x = block(x,32)\n",
    "        skip = skip + tfa.layers.SpectralNormalization(layers.Conv1D(gf_dim, 1, strides=1, padding='same'))(x[:,-data_len:])\n",
    "        x = block(x,16)\n",
    "        skip = skip + tfa.layers.SpectralNormalization(layers.Conv1D(gf_dim, 1, strides=1, padding='same'))(x[:,-data_len:])       \n",
    "        x = block(x,8)\n",
    "        skip = skip + tfa.layers.SpectralNormalization(layers.Conv1D(gf_dim, 1, strides=1, padding='same'))(x[:,-data_len:])       \n",
    "        x = block(x,6)\n",
    "        skip = skip + tfa.layers.SpectralNormalization(layers.Conv1D(gf_dim, 1, strides=1, padding='same'))(x)      \n",
    "        y = end_block(skip, data_channel) \n",
    "        model = tf.keras.Model(inputs, y)\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0IKnaCtg6WE"
   },
   "source": [
    "### The Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dw2tPLmk2pEP"
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model(data_len, data_channel, choice, df_dim, attn, bn=True, act=layers.LeakyReLU, cumsum=False, skip_con=False, diff=False):\n",
    "    if choice == 'sagan':\n",
    "        d_kernel = 5 \n",
    "        max_channel = 160       \n",
    "        inputs = layers.Input(shape= (data_len, data_channel))\n",
    "        x = inputs\n",
    "        if cumsum: \n",
    "          x_cumsum = tf.math.cumsum(x, axis=1)  \n",
    "          x = tf.concat([x, x_cumsum],axis=-1)\n",
    "        x = D_res_start_block(x, min(max_channel,df_dim), d_kernel, bn=bn, act=act)  \n",
    "        x = D_res_block(x, min(max_channel,df_dim * 2), d_kernel, bn=bn, act=act)  \n",
    "        x = multi_head_attn_block(x, hidden_ratio=attn[0], g_ratio=attn[1], nH=attn[2], sparse=attn[3], GD='D')  \n",
    "        x = D_res_block(x, min(max_channel,df_dim * 4), d_kernel, bn=bn, act=act)  \n",
    "        x = D_res_block(x, min(max_channel,df_dim * 8), d_kernel, bn=bn, act=act)  \n",
    "        x = D_res_block(x, min(max_channel,df_dim * 16), d_kernel, bn=bn, act=act)  \n",
    "        x = D_res_keep_block(x, d_kernel, bn=bn, act=act)\n",
    "        if bn: x = layers.BatchNormalization()(x)\n",
    "        x = act()(x)\n",
    "        x = tf.reduce_sum(x, [1])\n",
    "        y = tfa.layers.SpectralNormalization(layers.Dense(1))(x)\n",
    "        model = tf.keras.Model(inputs, y)\n",
    "\n",
    "    elif choice == 'transgan':\n",
    "        def mlp_block(x, ratio=4):\n",
    "          x0 = x\n",
    "          num_channels = x.shape[-1]\n",
    "          x = tfa.layers.SpectralNormalization(layers.Conv1D(num_channels * ratio, 1, strides=1, padding='same'))(x)\n",
    "          x = act()(x)\n",
    "          x = tfa.layers.SpectralNormalization(layers.Conv1D(num_channels, 1, strides=1, padding='same'))(x)\n",
    "          return x0 + x\n",
    "        def block(x):\n",
    "          x = layers.BatchNormalization()(x)\n",
    "          x = multi_head_attn_block(x, hidden_ratio=attn[0], g_ratio=attn[1], nH=attn[2], sparse=attn[3], GD='D')\n",
    "          x = layers.BatchNormalization()(x)\n",
    "          x = mlp_block(x)       \n",
    "          return x\n",
    "        inputs = layers.Input(shape= (data_len, data_channel))\n",
    "        x = inputs\n",
    "        if cumsum: \n",
    "          x_cumsum = tf.math.cumsum(x, axis=1)  \n",
    "          x = tf.concat([x, x_cumsum],axis=-1)\n",
    "        elif diff: \n",
    "          x_diff = tf.concat([tf.zeros_like(x[:,:1]), x[:,1:]-x[:,:-1]],axis=1)\n",
    "          x = tf.concat([x, x_diff],axis=-1)\n",
    "        x = tfa.layers.SpectralNormalization(layers.Conv1D(df_dim, 1, strides=1, padding='same'))(x) \n",
    "        if skip_con: skip = tfa.layers.SpectralNormalization(layers.Conv1D(df_dim, 1))(x)      \n",
    "        x = block(x)  \n",
    "        if skip_con: skip += tfa.layers.SpectralNormalization(layers.Conv1D(df_dim, 1))(x)      \n",
    "        x = block(x) \n",
    "        if skip_con: skip += tfa.layers.SpectralNormalization(layers.Conv1D(df_dim, 1))(x)      \n",
    "        x = block(x)\n",
    "        if skip_con: skip += tfa.layers.SpectralNormalization(layers.Conv1D(df_dim, 1))(x)      \n",
    "        x = block(x)\n",
    "        if skip_con: skip += tfa.layers.SpectralNormalization(layers.Conv1D(df_dim, 1))(x)      \n",
    "        x = block(x)\n",
    "        if skip_con: skip += tfa.layers.SpectralNormalization(layers.Conv1D(df_dim, 1))(x)      \n",
    "        if skip_con: \n",
    "        \tx = tf.keras.layers.Flatten()(skip)\n",
    "        else:\n",
    "        \tx = tf.keras.layers.Flatten()(x)\n",
    "        y = tfa.layers.SpectralNormalization(layers.Dense(1))(x)\n",
    "        model = tf.keras.Model(inputs, y)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "## Define the loss and optimizers\n",
    "\n",
    "Define loss functions and optimizers for both models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psQfmXxYKU3X"
   },
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def ls_loss(a,b):\n",
    "  return tf.reduce_mean(tf.nn.l2_loss(a - b)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKY_iPSPNWoj"
   },
   "source": [
    "### Discriminator loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkMNfBWlT-PV"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output, choice, generated_images = None, images= None, discriminator = None):\n",
    "    if choice == 'dcgan': # standard loss\n",
    "        real_loss = cross_entropy(tf.ones_like(real_output), real_output) \n",
    "        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "        total_loss = real_loss + fake_loss\n",
    "    elif choice == 'wgan_gp':\n",
    "        real_loss = -tf.reduce_mean(real_output)\n",
    "        fake_loss = tf.reduce_mean(fake_output)\n",
    "\n",
    "        alpha = tf.random.uniform([images.shape[0],1,1])\n",
    "        interpolates = alpha*images + ((1-alpha)*generated_images)\n",
    "        disc_interpolates = discriminator(interpolates, training=True)\n",
    "        gradients = tf.gradients(disc_interpolates, interpolates)[0]\n",
    "        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1]))\n",
    "        gradient_penalty = tf.reduce_mean((slopes-1)**2)\n",
    "        lam = 10\n",
    "        total_loss = real_loss + fake_loss + lam*gradient_penalty\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jd-3GCUEiKtv"
   },
   "source": [
    "### Generator loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90BIcCKcDMxz"
   },
   "outputs": [],
   "source": [
    "def generator_loss(fake_output, choice, real_output = None):\n",
    "    if choice == 'wgan_gp':\n",
    "        loss = -tf.reduce_mean(fake_output)\n",
    "    elif choice == 'dcgan':\n",
    "        loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgIc7i0th_Iu"
   },
   "source": [
    "### Optimizers\n",
    "The discriminator and the generator optimizers are different since we will train two networks separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWCn_PVdEJZ7"
   },
   "outputs": [],
   "source": [
    "def generator_optimizer_fun(choice):\n",
    "    if choice == 'wgan_gp':\n",
    "        return tf.keras.optimizers.Adam(1e-4, beta_1=0., beta_2=0.9)            \n",
    "    elif choice == 'quantgan':\n",
    "        return tf.keras.optimizers.Adam(1e-4, beta_1=0.0, beta_2=0.9)\n",
    "\n",
    "def discriminator_optimizer_fun(choice):\n",
    "    if choice == 'wgan_gp':\n",
    "        return tf.keras.optimizers.Adam(5*1e-4, beta_1=0., beta_2=0.9)\n",
    "    elif choice == 'quantgan':\n",
    "        return tf.keras.optimizers.Adam(3e-4, beta_1=0.0, beta_2=0.9)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rw1fkAczTQYh"
   },
   "source": [
    "## Define the training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXd1fBv40bmY"
   },
   "outputs": [],
   "source": [
    "# alpha = 0.6\n",
    "# 0, 1\n",
    "score_function1, score_function2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3t5ibNo05jCB"
   },
   "outputs": [],
   "source": [
    "def train_step_raw(images, generator, discriminator, generator_optimizer, discriminator_optimizer, choice, pre_trained=None):\n",
    "    noise = tf.random.normal((images.shape[0],) + generator.input_shape[1:])\n",
    "\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      if choice == 'wgan_gp':\n",
    "        disc_loss = discriminator_loss(real_output, fake_output, choice, generated_images = generated_images, images= images, discriminator=discriminator)\n",
    "      else:\n",
    "        disc_loss = discriminator_loss(real_output, fake_output, choice)\n",
    "\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)   \n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    noise = tf.random.normal((images.shape[0],) + generator.input_shape[1:])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output, choice)\n",
    "\n",
    "      if pre_trained is not None: \n",
    "        pre_trained_output = pre_trained(generated_images, training=True) # 0 or 1 from D2\n",
    "        gen_loss += generator_loss(pre_trained_output, choice)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)     \n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2M7LmLtGEMQJ"
   },
   "outputs": [],
   "source": [
    "def train(dataset, batchsize, generator, discriminator, G_opt, D_opt, test, epochs, choice, pre_trained=None):\n",
    "  train_step = tf.function(train_step_raw)\n",
    "  dataset = dataset.astype('float32')\n",
    "  image_batch = tf.Variable(initial_value=np.zeros((batchsize,)+dataset.shape[1:]), trainable=False, dtype='float32')\n",
    "  datasize = dataset.shape[0]\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    G_list = []; D_list = []\n",
    "\n",
    "    for i in range(datasize//batchsize):\n",
    "      image_batch.assign(dataset[np.random.choice(datasize, size=batchsize, replace=False)])\n",
    "      gen_loss, disc_loss = train_step(image_batch, generator, discriminator, G_opt, D_opt, choice, pre_trained)\n",
    "      G_list.append(gen_loss)\n",
    "      D_list.append(disc_loss) \n",
    "\n",
    "    G_loss.append(np.mean(G_list))\n",
    "    D_loss.append(np.mean(D_list))\n",
    "\n",
    "    for fun, seed in test:\n",
    "      fun(generator, epoch + 1, seed) \n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aFF7Hk3XdeW"
   },
   "source": [
    "### Generate and save images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FN2nZStc1lkm"
   },
   "outputs": [],
   "source": [
    "def plot_loss(G_loss, D_loss):\n",
    "  fig = plt.figure(figsize=(4,4)) \n",
    "  fig.suptitle('Development of training losses during training')\n",
    "  if isinstance(D_loss,np.ndarray) and (len(D_loss.shape)>1): \n",
    "    [plt.plot(D_loss_i, label='Discriminator loss') for D_loss_i in D_loss]\n",
    "  else:\n",
    "    plt.plot(D_loss, label='Discriminator loss')\n",
    "  plt.plot(G_loss, label='Generator loss')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOQugGsshKG0"
   },
   "source": [
    "## Quant GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agB6IdCmWbkF"
   },
   "outputs": [],
   "source": [
    "# official\n",
    "\n",
    "prelu_kwargs = {'alpha_initializer':tf.keras.initializers.Constant(value=0.25), 'shared_axes':[1,2]}\n",
    "\n",
    "def TemporalBlock(x, skip, output_dim, hidden_dim_skip, kernel_size, dilation, num_inner_blocks=2, batch_norm=False, **kwargs):\n",
    "    drop_left = num_inner_blocks * (kernel_size - 1) * dilation\n",
    "    in_dim = x.shape[-1]\n",
    "\n",
    "    x_0 = x\n",
    "\n",
    "    for i in range(num_inner_blocks):\n",
    "      init = tf.keras.initializers.RandomUniform(minval=-1/np.sqrt(x.shape[2]*kernel_size), maxval=1/np.sqrt(x.shape[2]*kernel_size))\n",
    "      inits = {'kernel_initializer': init, 'bias_initializer': init}\n",
    "      x = tfa.layers.SpectralNormalization(layers.Conv1D(output_dim, kernel_size, strides=1, \n",
    "                                                       padding='valid', dilation_rate=dilation, **inits, **kwargs))(x)\n",
    "      x = layers.PReLU(**prelu_kwargs)(x) \n",
    "      if batch_norm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    if in_dim != output_dim: \n",
    "      init = tf.keras.initializers.RandomUniform(minval=-1/np.sqrt(x_0.shape[2]*1), maxval=1/np.sqrt(x_0.shape[2]*1))\n",
    "      inits = {'kernel_initializer': init, 'bias_initializer': init}\n",
    "      x_0 = tfa.layers.SpectralNormalization(layers.Conv1D(output_dim, 1, strides=1, padding='same', **inits, **kwargs))(x_0) \n",
    "    \n",
    "    x = x + x_0[:,drop_left:]\n",
    "    init = tf.keras.initializers.RandomUniform(minval=-1/np.sqrt(x.shape[2]*1), maxval=1/np.sqrt(x.shape[2]*1))\n",
    "    inits = {'kernel_initializer': init, 'bias_initializer': init}\n",
    "    skip = skip + tfa.layers.SpectralNormalization(layers.Conv1D(hidden_dim_skip, 1, strides=1, padding='same', **inits, **kwargs))(x[:, -skip.shape[1]:])\n",
    "    \n",
    "    return layers.PReLU(**prelu_kwargs)(x), skip\n",
    "\n",
    "def TCN(x, output_dim, hidden_dims, hidden_dim_skip=50, \n",
    "        kernel_size=2, num_inner_blocks=2, dilation_factor=2, batch_norm=False, **kwargs):\n",
    "    drop_left =  sum(num_inner_blocks * (kernel_size - 1) * dilation_factor**min(i,6) for i in range(len(hidden_dims)))\n",
    "    skip = tf.zeros([tf.shape(x)[0], x.shape[1] - drop_left, hidden_dim_skip])\n",
    "    \n",
    "    x, skip = TemporalBlock(x, skip, hidden_dims[0], hidden_dim_skip, 1, 1, num_inner_blocks=2, batch_norm=batch_norm, **kwargs)\n",
    "    for i, h in enumerate(hidden_dims):\n",
    "        x, skip = TemporalBlock(x, skip, h, hidden_dim_skip, kernel_size, dilation_factor**min(i,6), num_inner_blocks=2, batch_norm=batch_norm, **kwargs)\n",
    "    \n",
    "    y = layers.PReLU(**prelu_kwargs)(skip) \n",
    "    init = tf.keras.initializers.RandomUniform(minval=-1/np.sqrt(y.shape[2]*1), maxval=1/np.sqrt(y.shape[2]*1))\n",
    "    inits = {'kernel_initializer': init, 'bias_initializer': init}\n",
    "    y = tfa.layers.SpectralNormalization(layers.Conv1D(hidden_dim_skip, 1, strides=1, padding='same', **inits, **kwargs))(y) \n",
    "    y = layers.PReLU(**prelu_kwargs)(y) \n",
    "    y = tfa.layers.SpectralNormalization(layers.Conv1D(output_dim, 1, strides=1, padding='same', **inits, **kwargs))(y) \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptQ5V46WwS5q"
   },
   "outputs": [],
   "source": [
    "def G_quant_gan(data_len, data_channel, noise_len, noise_channel, gf_dim = 50): \n",
    "    inputs = layers.Input(shape= (noise_len, noise_channel))\n",
    "    if noise_len-data_len == 126:\n",
    "      n_block = 6\n",
    "    elif noise_len-data_len == (126+256):\n",
    "      n_block = 8\n",
    "    else:\n",
    "      raise Exception\n",
    "    y = TCN(inputs, data_channel, hidden_dims = n_block*[gf_dim], hidden_dim_skip=gf_dim, batch_norm=True)\n",
    "    model = tf.keras.Model(inputs, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jiTrOoDnzAB3"
   },
   "outputs": [],
   "source": [
    "def D_quant_gan(data_len, data_channel, df_dim = 50, cumsum=False): \n",
    "    inputs = layers.Input(shape= (data_len, data_channel))\n",
    "    x = inputs\n",
    "    if cumsum: \n",
    "      x_cumsum = tf.math.cumsum(x, axis=1)  \n",
    "      x = tf.concat([x, x_cumsum],axis=-1)\n",
    "        \n",
    "    y = TCN(x, 1, hidden_dims = 6*[df_dim], hidden_dim_skip=df_dim, batch_norm=False)\n",
    "    y = layers.Flatten()(y)\n",
    "    model = tf.keras.Model(inputs, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BwhNqQ853OM"
   },
   "source": [
    "## Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aria\n",
    "from aria.components.dataframe_regressor import DataFrameRegressor\n",
    "from aria.components.fourier_seasonality import FourierSeasonality\n",
    "from aria.components.linear_trend import LinearTrend\n",
    "from aria.components.linear_trend_changepoints import LinearTrendChangepoints\n",
    "from aria.datasets.example import load_data\n",
    "from aria.models.forward_stepwise_selector import ForwardStepwiseSelector\n",
    "from aria.models.linear_lasso import LinearLassoModel\n",
    "from aria.models.linear_ridge import LinearRidgeModel\n",
    "from aria.utils.evaluation import smape, bic\n",
    "\n",
    "#logging.getLogger().setLevel('DEBUG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r3htnJqfhy4G"
   },
   "outputs": [],
   "source": [
    "from pandas_datareader import data as pdr\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "\n",
    "def download_prices_from_yf(ticker, start_date, end_date, label_list):\n",
    "    yf.pdr_override()\n",
    "    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    return pdr.get_data_yahoo(ticker, start_date, end_date)[label_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wet7q8VxmHUj",
    "outputId": "2c0b9ff9-3896-42f6-941e-65fffd50cdce"
   },
   "outputs": [],
   "source": [
    "sp500 = download_prices_from_yf(start_date='2009-05-01', end_date='2021-01-01',\n",
    "              ticker='^GSPC', label_list=['Adj Close'])\n",
    "real_close = sp500['Adj Close']\n",
    "real_return = np.log(real_close).diff().dropna()\n",
    "real_mean = real_return.mean()\n",
    "real_std = real_return.std()\n",
    "standard_return = (real_return-real_mean)/real_std\n",
    "sequence_data = np.asarray(standard_return) \n",
    "\n",
    "scaled_sp500_diff = sp500.rename(columns = {\"Date\":\"ds\", \"Adj Close\": \"y\"}).iloc[1:, :]\n",
    "scaled_sp500_diff['y'] = sequence_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_sp500_diff.iloc[-100:, :].y.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_sp500_diff.y.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scaled_sp500_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Date']\n",
    "df[df.index.name] = df.index\n",
    "df = df.rename(columns = {\"Date\":\"ds\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scaled_sp500_diff\n",
    "\n",
    "n = int(df.shape[0] * 0.8)\n",
    "train_df, test_df = df.iloc[:n], df.iloc[n:]\n",
    "\n",
    "components = [\n",
    "    LinearTrend(),\n",
    "    LinearTrendChangepoints(),\n",
    "    FourierSeasonality(7, 3),\n",
    "    FourierSeasonality(30.4375, 5),\n",
    "    FourierSeasonality(365.25, 10),\n",
    "]\n",
    "\n",
    "print('RIDGE') \n",
    "m = LinearRidgeModel(components=components)\n",
    "m.fit(train_df)\n",
    "print(pd.DataFrame(index=m.get_features_list(), data=m.get_params()))\n",
    "print('SMAPE', smape(test_df['y'], m.predict(test_df)['yhat']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge (normal prior) has a lower SMAPE (1.71 vs 1.77) hence we use Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_coef = pd.DataFrame({\"feature\":m.get_features_list(), \"coef\": m.get_params()})\n",
    "m_coef[m_coef.feature.str.contains('FS')].plot()\n",
    "season_coef = m_coef[m_coef.feature.str.contains('FS')]\n",
    "score = np.abs(np.max(season_coef))/ season_coef.apply(lambda x: np.abs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s for s in \n",
    " df[df.City.str.contains('ville',case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_coef.get_loc(m_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_coef."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pr4Jahh0mu-h"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5ZemO3e1FqB"
   },
   "outputs": [],
   "source": [
    "def recursive_simulator(generator, seed, data_len, noise_len, steps):\n",
    "  generated_list = []\n",
    "  for i in range(steps):\n",
    "    generated_image = generator(seed[:,i*data_len:i*data_len+noise_len], training=False)\n",
    "    generated_list.append(generated_image)\n",
    "  return np.concatenate(generated_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ke9HlfZ6lu4"
   },
   "outputs": [],
   "source": [
    "def several_returns(x, days = [5, 22, 64, 128, 256]):\n",
    "  cumsum_x = np.cumsum(x,axis=1)\n",
    "  returns = {'1': np.reshape(x,(-1,))}\n",
    "  for each in days:\n",
    "    if each < x.shape[1]:\n",
    "      returns[str(each)] = np.reshape(cumsum_x[:,each:] - cumsum_x[:,:-each],(-1,))\n",
    "  return returns\n",
    "\n",
    "def moments(x):\n",
    "  return {#'mean':np.mean(x, axis=1), 'std':np.std(x, axis=1), \n",
    "          'skew':scipy.stats.skew(x, axis=1), 'kurt':scipy.stats.kurtosis(x, axis=1)}\n",
    "\n",
    "def var_short_fall(x, alpha=0.05):\n",
    "  var = np.quantile(x, alpha, axis=1) \n",
    "  es = np.where(x <= var[:,np.newaxis], x, 0) \n",
    "  es = np.mean(es, axis=1)/alpha\n",
    "  return {'VaR':var, 'ES':es}\n",
    "\n",
    "def acf_lev(x, max_lag=64):\n",
    "  x_mean = x - np.mean(x)\n",
    "  abs_mean = np.abs(x) - np.mean(np.abs(x))\n",
    "  sq_mean = x**2 - np.mean(x**2)\n",
    "  x_var = np.mean(x_mean**2)\n",
    "  abs_var = np.mean(abs_mean**2)\n",
    "  sq_var = np.mean(sq_mean**2)\n",
    "  acf = [np.mean(x_mean[:,i:]*x_mean[:,:-i],axis=1) for i in range(1,max_lag+1)]\n",
    "  acf = np.array(acf).T/x_var\n",
    "  \n",
    "  abs_acf = [np.mean(abs_mean[:,i:]*abs_mean[:,:-i],axis=1) for i in range(1,max_lag+1)]\n",
    "  abs_acf = np.array(abs_acf).T/abs_var\n",
    "  \n",
    "  sq_acf = [np.mean(sq_mean[:,i:]*sq_mean[:,:-i],axis=1) for i in range(1,max_lag+1)]\n",
    "  sq_acf = np.array(sq_acf).T/sq_var\n",
    "  \n",
    "  lev = [np.mean(sq_mean[:,i:]*x_mean[:,:-i],axis=1) for i in range(1,max_lag+1)]\n",
    "  lev = np.array(lev).T/np.sqrt(x_var*sq_var)\n",
    "  \n",
    "  return {'acf':acf, 'abs_acf':abs_acf, 'sq_acf':sq_acf, 'lev':lev}\n",
    "\n",
    "def set_style(ax):\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKnTK95GqyLd"
   },
   "outputs": [],
   "source": [
    "class Loss():\n",
    "  def __init__(self, x_real, real_mean, real_std, max_lag, long_seq=False, simple_loss=True, corr_std=True):\n",
    "    self.days = [5,20,50,100,200]\n",
    "    self.real_mean = real_mean\n",
    "    self.real_std = real_std\n",
    "    self.max_lag = max_lag\n",
    "    self.x_real = x_real * self.real_std + self.real_mean\n",
    "    self.real_statistics = self.statistics(self.x_real)\n",
    "    self.long_seq = long_seq\n",
    "    self.loss_history = None\n",
    "    self.simple_loss = simple_loss\n",
    "    self.corr_std = corr_std  \n",
    "  def statistics(self, x):\n",
    "    return {'returns': several_returns(x,days=self.days), \n",
    "            'moments': moments(x),\n",
    "            'var': var_short_fall(x),\n",
    "            'acf': acf_lev(x, max_lag=self.max_lag)}\n",
    "  def compute_loss(self, x_fake):\n",
    "    if self.real_mean ==0:\n",
    "      x_fake = (x_fake)/np.std(x_fake)\n",
    "    else:\n",
    "      x_fake = (x_fake - np.mean(x_fake))/np.std(x_fake)\n",
    "    self.x_fake = x_fake * self.real_std + self.real_mean\n",
    "    self.fake_statistics = self.statistics(self.x_fake)\n",
    "    loss_summary = {}\n",
    "    loss_summary['returns_dist'] = {each: scipy.stats.wasserstein_distance(self.fake_statistics['returns'][each],\n",
    "                                                                self.real_statistics['returns'][each])\n",
    "                               for each in self.fake_statistics['returns']}\n",
    "    if not (self.simple_loss or self.long_seq): \n",
    "      loss_summary['moments_dist'] = {each: scipy.stats.wasserstein_distance(self.fake_statistics['moments'][each],\n",
    "                                                                self.real_statistics['moments'][each])\n",
    "                               for each in self.fake_statistics['moments']} \n",
    "    loss_summary['moments_diff'] = {each: np.abs(np.mean(self.fake_statistics['moments'][each])-\n",
    "                                                 np.mean(self.real_statistics['moments'][each]))\n",
    "                               for each in self.fake_statistics['moments']}\n",
    "    if not (self.simple_loss or self.long_seq): \n",
    "      loss_summary['var_dist'] = {each: scipy.stats.wasserstein_distance(self.fake_statistics['var'][each],\n",
    "                                                                self.real_statistics['var'][each])\n",
    "                               for each in self.fake_statistics['var']}\n",
    "    if not self.simple_loss: \n",
    "      loss_summary['var_diff'] = {each: np.abs(np.mean(self.fake_statistics['var'][each])-\n",
    "                                                 np.mean(self.real_statistics['var'][each]))\n",
    "                               for each in self.fake_statistics['var']} \n",
    "    loss_summary['acf_l2'] = {each: np.linalg.norm(np.mean(self.fake_statistics['acf'][each],axis=0)-\n",
    "                                                 np.mean(self.real_statistics['acf'][each],axis=0))\n",
    "                               for each in self.fake_statistics['acf']}                                                        \n",
    "    self.loss_summary = loss_summary\n",
    "    if self.loss_history is None:\n",
    "      self.loss_history = {k0:{k1:[v1] for k1,v1 in v0.items()} for k0, v0 in self.loss_summary.items()}\n",
    "    else:\n",
    "      [v1.append(self.loss_summary[k0][k1]) for k0, v0 in self.loss_history.items() for k1,v1 in v0.items()]\n",
    "\n",
    "  def plot_loss(self, figsize=None):\n",
    "    num_of_plots = sum(len(v0) for k0, v0 in self.loss_summary.items())\n",
    "    if figsize is None: figsize = (16, 4*int(np.ceil(num_of_plots/4)))\n",
    "    fig, axes = plt.subplots(int(np.ceil(num_of_plots/4)), 4, figsize=figsize)\n",
    "    axes = np.reshape(axes, (-1,))\n",
    "    i = 0\n",
    "    for k0, v0 in self.loss_history.items(): \n",
    "      for k1,v1 in v0.items():\n",
    "        axes[i].plot(v1) \n",
    "        axes[i].title.set_text(k0+'_'+k1)\n",
    "        i +=1\n",
    "  \n",
    "  def plot_moments(self, figsize=None):\n",
    "    if figsize is None: figsize = (8, 4)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    titles = [#'Mean', 'Standard deviation', \n",
    "              'Skewness', 'Kurtosis']\n",
    "\n",
    "    for i, each in enumerate(self.fake_statistics['moments']):\n",
    "      axes[i].boxplot([self.real_statistics['moments'][each], self.fake_statistics['moments'][each]])\n",
    "      axes[i].set_xticklabels(['Historical', 'Synthetic'])\n",
    "      axes[i].grid()\n",
    "      set_style(axes[i])   \n",
    "      axes[i].title.set_text(titles[i])\n",
    "    # fig.tight_layout()\n",
    "\n",
    "  def plot_var(self, figsize=None):\n",
    "    if figsize is None: figsize = (8, 4)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    titles = ['Value at risk', 'Expected shortfall']\n",
    "\n",
    "    for i, each in enumerate(self.fake_statistics['var']):\n",
    "      axes[i].boxplot([self.real_statistics['var'][each], self.fake_statistics['var'][each]])\n",
    "      axes[i].set_xticklabels(['Historical', 'Synthetic'])\n",
    "      axes[i].grid()\n",
    "      set_style(axes[i])   \n",
    "      axes[i].title.set_text(titles[i])\n",
    "\n",
    "  def plot_corr(self, figsize=None, corr_std = None):\n",
    "    if corr_std is not None: self.corr_std = corr_std \n",
    "    if figsize is None: figsize = (16, 4)\n",
    "    fig, axes = plt.subplots(1, 4, figsize=figsize)\n",
    "    titles = ['Serial ACF', 'ACF of absolute log-returns', 'ACF of square log-returns', 'Leverage effect']\n",
    "    \n",
    "    for i, each in enumerate(self.fake_statistics['acf']):\n",
    "      num = np.arange(1,1+self.max_lag)\n",
    "      acf_real_mean = np.mean(self.real_statistics['acf'][each],axis=0)\n",
    "      axes[i].plot(num, acf_real_mean, label='Historical')\n",
    "      acf_fake_mean = np.mean(self.fake_statistics['acf'][each],axis=0)\n",
    "      axes[i].plot(num, acf_fake_mean, label='Generated', alpha=0.8)\n",
    "      axes[i].grid()\n",
    "      set_style(axes[i])   \n",
    "      axes[i].set_xlabel('Lags')\n",
    "      # axes[i].set_ylabel('ACF')\n",
    "      axes[i].legend()\n",
    "      axes[i].title.set_text(titles[i])\n",
    "      if self.corr_std: \n",
    "        acf_fake_std = np.std(self.fake_statistics['acf'][each],axis=0)\n",
    "        ub = acf_fake_mean + acf_fake_std\n",
    "        lb = acf_fake_mean - acf_fake_std\n",
    "        axes[i].fill_between(num, ub, lb,\n",
    "              color='C1', alpha=.3)\n",
    "        acf_real_std = np.std(self.real_statistics['acf'][each],axis=0)\n",
    "        ub = acf_real_mean + acf_real_std\n",
    "        lb = acf_real_mean - acf_real_std\n",
    "        axes[i].fill_between(num, ub, lb,\n",
    "              color='C0', alpha=.1)\n",
    "  \n",
    "  def plot_curve(self, figsize=None, number = 1):\n",
    "    if figsize is None: figsize = (4, 4)\n",
    "    fig, axes = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    for i in range(number):\n",
    "        axes.plot(np.cumsum(self.x_real[i]),'C0',label='Historic' if i==0 else None)\n",
    "        axes.plot(np.cumsum(self.x_fake[i]),'C1',label='Generated' if i==0 else None)\n",
    "    axes.grid()\n",
    "    axes.legend()\n",
    "    set_style(axes)\n",
    "    axes.title.set_text('Sample curves')\n",
    "\n",
    "  def plot_hists(self, figsize=None, textbox=True):\n",
    "    if128 = 1 if str(self.days[3]) in self.fake_statistics['returns'] else 0\n",
    "    if figsize is None: figsize = (6*(2+if128), 4)\n",
    "    fig, axes = plt.subplots(1, 2+if128, figsize=figsize)\n",
    "    \n",
    "    self.one_hist(self.real_statistics['returns']['1'], self.fake_statistics['returns']['1'], axes[0], textbox)\n",
    "    self.one_hist(self.real_statistics['returns'][str(self.days[2])], self.fake_statistics['returns'][str(self.days[2])], axes[1], textbox)\n",
    "    if if128:\n",
    "      self.one_hist(self.real_statistics['returns'][str(self.days[3])], self.fake_statistics['returns'][str(self.days[3])], axes[2], textbox)\n",
    "  \n",
    "  def plot_all(self):\n",
    "    self.plot_hists()\n",
    "    self.plot_corr()\n",
    "    self.plot_moments()\n",
    "    self.plot_var()\n",
    "  \n",
    "  def one_hist(self, x_real, x_fake, ax, textbox, clip = None):\n",
    "    if clip is not None:\n",
    "      x_real_clipped = np.clip(x_real,*clip)\n",
    "      x_fake_clipped = np.clip(x_fake,*clip)\n",
    "    else:\n",
    "      x_real_clipped = x_real\n",
    "      x_fake_clipped = x_fake\n",
    "\n",
    "    ax.hist([x_real_clipped, x_fake_clipped],\n",
    "            bins=50, alpha=1, density=True, label=['Historical', 'Synthetic'])\n",
    "    ax.grid()\n",
    "    set_style(ax)\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('PDF')\n",
    "    ax.set_xlabel('Log-Return')\n",
    "\n",
    "    if textbox:\n",
    "      self.text_box(x_real, ax, 0.05, 'Historical')\n",
    "      self.text_box(x_fake, ax, 0.75, 'Generated')\n",
    "\n",
    "  def text_box(self, x, ax, pos, title):\n",
    "    textstr = '\\n'.join(['{}','mean={:.2f}', 'std={:.2f}', 'skew={:.2f}', 'kurt={:.2f}']).format(\n",
    "        title, np.mean(x), np.std(x), scipy.stats.skew(x), scipy.stats.kurtosis(x))        \n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    ax.text(pos, 0.70, textstr,\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=14,\n",
    "        verticalalignment='top',\n",
    "        bbox=props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQZrQXjin2e3"
   },
   "source": [
    "## Training\n",
    "\n",
    "There is no guarantee on the number of training epochs. Usually check the results every 100 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_SCsmWcF9az"
   },
   "source": [
    "### TAGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PhBlNh-pF9a0"
   },
   "outputs": [],
   "source": [
    "choice = 'sagan' \n",
    "opt_choice = 'wgan_gp'\n",
    "loss_choice = 'wgan_gp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Nj_w8ivF9a1"
   },
   "outputs": [],
   "source": [
    "gf_dim = 64\n",
    "g_attn = (1,1,1,False)\n",
    "df_dim = 8\n",
    "d_attn = (1,1,1,False)\n",
    "g_act = layers.ReLU\n",
    "d_act = layers.LeakyReLU\n",
    "d_bn = True\n",
    "cumsum = False\n",
    "g_kernel=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yaddZjR5F9a2",
    "outputId": "1085a528-42d8-40ab-e064-aa5ba7662ed4"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "data_len = 128\n",
    "data_channel = 1\n",
    "noise_len = 254\n",
    "noise_channel = 3\n",
    "\n",
    "data = np.array([standard_return[t:t + len(standard_return)-data_len] for t in range(0, data_len)]).T[:,:,np.newaxis]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ee8W4x3EF9a2"
   },
   "outputs": [],
   "source": [
    "def compare_test(model, epoch, test_input):\n",
    "  generated_image = generator(test_input, training=False)[:,:,0]\n",
    "  loss_obj_training.compute_loss(generated_image)\n",
    "  if (epoch%10 == 0) or (epoch == 1):\n",
    "    display.clear_output()\n",
    "    loss_obj_training.plot_loss()\n",
    "    loss_obj_training.plot_corr()\n",
    "    plt.show()\n",
    "\n",
    "num_examples_to_generate_2 = 512\n",
    "seed2 = tf.random.normal([num_examples_to_generate_2, noise_len, noise_channel])\n",
    "\n",
    "test = [[compare_test,seed2]]\n",
    "\n",
    "sample_data_2 = data[np.random.choice(data.shape[0], num_examples_to_generate_2, replace=False),:,0]\n",
    "loss_obj_training = Loss(sample_data_2, real_mean=real_mean, real_std=real_std, max_lag=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-XY6pd3F9a3"
   },
   "outputs": [],
   "source": [
    "generator = make_generator_model(data_len, data_channel, noise_len, noise_channel, gf_dim=gf_dim, choice=choice, attn=g_attn, act=g_act, g_kernel=g_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXLsc-12F9a4"
   },
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model(data_len, data_channel, choice, df_dim=df_dim, attn=d_attn, bn=d_bn, act=d_act, cumsum=cumsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ytz5a0xF9a5"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = generator_optimizer_fun(opt_choice)\n",
    "discriminator_optimizer = discriminator_optimizer_fun(opt_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPJajgvYF9a5"
   },
   "outputs": [],
   "source": [
    "G_loss = []; D_loss = [];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBWa5kEwF9a_"
   },
   "source": [
    "#### 200e training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1011
    },
    "id": "sdQXB_R_F9a_",
    "outputId": "2b56af6b-5ef2-402b-f0ec-2a68ce5733b9"
   },
   "outputs": [],
   "source": [
    "train(data, BATCH_SIZE, generator, discriminator, generator_optimizer, discriminator_optimizer, test, 200, loss_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmSG6_ddF9bA"
   },
   "source": [
    "#### 200e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYbNYd2UF9bC"
   },
   "outputs": [],
   "source": [
    "num_examples_to_generate_4 = 512\n",
    "steps = 20\n",
    "total_data_length = data_len * steps\n",
    "total_noise_length = total_data_length - data_len + noise_len\n",
    "seed4 = tf.random.normal([num_examples_to_generate_4, total_noise_length, noise_channel])\n",
    "sample_data_4 = np.array([sequence_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J9Qte9xeF9bC",
    "outputId": "c585697f-eb50-4fc5-9c74-9df9d9764cee"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "generated_image4 = recursive_simulator(generator, seed4, data_len, noise_len, steps)[:,:,0]\n",
    "generated_image4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ym6rIrK4F9bD"
   },
   "outputs": [],
   "source": [
    "loss_obj_long = Loss(sample_data_4, real_mean=real_mean, real_std=real_std, max_lag=250, simple_loss=False, long_seq=True)\n",
    "loss_obj_long.compute_loss(generated_image4)\n",
    "# loss_obj_long.plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vdul5jtBF9bE",
    "outputId": "342ca8d2-667f-4cc9-c7cd-769a82c7c8a9"
   },
   "outputs": [],
   "source": [
    "loss_obj_long.loss_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wON3c2yLF9bE"
   },
   "source": [
    "#### 300e training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1011
    },
    "id": "FC7TSDFrF9bE",
    "outputId": "1babe864-ac13-482a-c0fd-8c22e28c5d70"
   },
   "outputs": [],
   "source": [
    "train(data, BATCH_SIZE, generator, discriminator, generator_optimizer, discriminator_optimizer, test, 100, loss_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBXR_6x6F9bH"
   },
   "source": [
    "#### 300e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxbrDLK_F9bI"
   },
   "outputs": [],
   "source": [
    "num_examples_to_generate_4 = 512\n",
    "steps = 20\n",
    "total_data_length = data_len * steps\n",
    "total_noise_length = total_data_length - data_len + noise_len\n",
    "seed4 = tf.random.normal([num_examples_to_generate_4, total_noise_length, noise_channel])\n",
    "sample_data_4 = np.array([sequence_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fEwN2NzlF9bI",
    "outputId": "2534a39b-2081-4d4b-a6e0-43bdb4ac0224"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "generated_image4 = recursive_simulator(generator, seed4, data_len, noise_len, steps)[:,:,0]\n",
    "generated_image4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1293
    },
    "id": "EaxeOt4sF9bJ",
    "outputId": "d51b8ef1-cfe9-4783-86a7-d9f6f0930579"
   },
   "outputs": [],
   "source": [
    "loss_obj_long = Loss(sample_data_4, real_mean=real_mean, real_std=real_std, max_lag=250, simple_loss=False, long_seq=True)\n",
    "loss_obj_long.compute_loss(generated_image4)\n",
    "loss_obj_long.plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Nh-EEEoF9bJ",
    "outputId": "2cfc1020-6948-4929-b2f0-c7743d839b68"
   },
   "outputs": [],
   "source": [
    "loss_obj_long.loss_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7Ux1iB0F9bJ"
   },
   "source": [
    "#### 400e training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1011
    },
    "id": "ioluno-4F9bK",
    "outputId": "0196fc72-7044-4f50-86e4-2c713668ebf8"
   },
   "outputs": [],
   "source": [
    "train(data, BATCH_SIZE, generator, discriminator, generator_optimizer, discriminator_optimizer, test, 100, loss_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aurPYeD-F9bL"
   },
   "source": [
    "#### 400e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9LqBwbjF9bM"
   },
   "outputs": [],
   "source": [
    "num_examples_to_generate_4 = 512\n",
    "steps = 20\n",
    "total_data_length = data_len * steps\n",
    "total_noise_length = total_data_length - data_len + noise_len\n",
    "seed4 = tf.random.normal([num_examples_to_generate_4, total_noise_length, noise_channel])\n",
    "sample_data_4 = np.array([sequence_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6xJXFhuF9bM",
    "outputId": "f57f0fa6-73b2-44c1-b0f0-788fe9eb106b"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "generated_image4 = recursive_simulator(generator, seed4, data_len, noise_len, steps)[:,:,0]\n",
    "generated_image4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1296
    },
    "id": "9Ncn0TmbF9bN",
    "outputId": "aa9f54ae-978d-4f8e-fc9e-8c07777fe83b"
   },
   "outputs": [],
   "source": [
    "loss_obj_long = Loss(sample_data_4, real_mean=real_mean, real_std=real_std, max_lag=250, simple_loss=False, long_seq=True)\n",
    "loss_obj_long.compute_loss(generated_image4)\n",
    "loss_obj_long.plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DgsAaGfBF9bN",
    "outputId": "0f352e34-e4b7-4c8e-9e24-2ea3996bdd99"
   },
   "outputs": [],
   "source": [
    "loss_obj_long.loss_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wULO3BInEG71"
   },
   "source": [
    "#### 500e training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1011
    },
    "id": "2oZfd8OKEG72",
    "outputId": "23b183d3-6775-4b52-f82a-591b1e66926f"
   },
   "outputs": [],
   "source": [
    "train(data, BATCH_SIZE, generator, discriminator, generator_optimizer, discriminator_optimizer, test, 100, loss_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXD_htyREG75"
   },
   "source": [
    "#### 500e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecdbs9VLEG77"
   },
   "outputs": [],
   "source": [
    "num_examples_to_generate_4 = 512\n",
    "steps = 20\n",
    "total_data_length = data_len * steps\n",
    "total_noise_length = total_data_length - data_len + noise_len\n",
    "seed4 = tf.random.normal([num_examples_to_generate_4, total_noise_length, noise_channel])\n",
    "sample_data_4 = np.array([sequence_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tHEzxLYTEG78",
    "outputId": "07362b7e-f4a6-4969-fb38-7aa8152cad96"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "generated_image4 = recursive_simulator(generator, seed4, data_len, noise_len, steps)[:,:,0]\n",
    "generated_image4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1293
    },
    "id": "NOAtvELDEG78",
    "outputId": "b05d6361-e0ea-4993-b88c-4a6f1bf51243"
   },
   "outputs": [],
   "source": [
    "loss_obj_long = Loss(sample_data_4, real_mean=real_mean, real_std=real_std, max_lag=250, simple_loss=False, long_seq=True)\n",
    "loss_obj_long.compute_loss(generated_image4)\n",
    "loss_obj_long.plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z6g322U3EG79",
    "outputId": "5c8247ed-1f3a-4b0d-ee2a-d17e322a83fb"
   },
   "outputs": [],
   "source": [
    "loss_obj_long.loss_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJE7Xc2lbYBe"
   },
   "source": [
    "### QuantGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bW8tdkBGbYBg"
   },
   "outputs": [],
   "source": [
    "choice = 'quantgan' \n",
    "opt_choice = 'quantgan'\n",
    "loss_choice = 'dcgan'\n",
    "cumsum = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bXlmOc5ebYBh"
   },
   "outputs": [],
   "source": [
    "gf_dim = 80\n",
    "df_dim = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb-LfoGAbYBi",
    "outputId": "b83ff731-278f-42e9-b189-aa9ab251fefa"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "data_len = 127\n",
    "data_channel = 1\n",
    "noise_len = 253\n",
    "noise_channel = 3\n",
    "\n",
    "data = np.array([standard_return[t:t + len(standard_return)-data_len] for t in range(0, data_len)]).T[:,:,np.newaxis]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqbfqykVbYBj"
   },
   "outputs": [],
   "source": [
    "def compare_test(model, epoch, test_input):\n",
    "  generated_image = generator(test_input, training=False)[:,:,0]\n",
    "  loss_obj_training.compute_loss(generated_image)\n",
    "  if (epoch%10 == 0) or (epoch == 1):\n",
    "    display.clear_output()\n",
    "    loss_obj_training.plot_loss()\n",
    "    loss_obj_training.plot_corr()\n",
    "    plt.show()\n",
    "\n",
    "num_examples_to_generate_2 = 512\n",
    "seed2 = tf.random.normal([num_examples_to_generate_2, noise_len, noise_channel])\n",
    "\n",
    "test = [[compare_test,seed2]]\n",
    "\n",
    "sample_data_2 = data[np.random.choice(data.shape[0], num_examples_to_generate_2, replace=False),:,0]\n",
    "loss_obj_training = Loss(sample_data_2, real_mean=real_mean, real_std=real_std, max_lag=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Xp5_f6UbYBk"
   },
   "outputs": [],
   "source": [
    "generator = G_quant_gan(noise_len=noise_len, data_len=data_len, data_channel = data_channel, noise_channel = noise_channel, gf_dim=gf_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXiCwMZrbYBl"
   },
   "outputs": [],
   "source": [
    "discriminator = D_quant_gan(data_len = data_len, data_channel = data_channel, df_dim=df_dim, cumsum = cumsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4k9xOBUbYBn"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = generator_optimizer_fun(opt_choice)\n",
    "discriminator_optimizer = discriminator_optimizer_fun(opt_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6MLkOOxbYBo"
   },
   "outputs": [],
   "source": [
    "G_loss = []; D_loss = [];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOwGm3JwWA9w"
   },
   "source": [
    "#### 200e training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uFBqERMoWA9x",
    "outputId": "f0ece958-cb50-47e2-dad6-2cf7298690d7"
   },
   "outputs": [],
   "source": [
    "train(data, BATCH_SIZE, generator, discriminator, generator_optimizer, discriminator_optimizer, test, 200, loss_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pLoh7c9WA90"
   },
   "source": [
    "#### 200e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "hQX6c2i2WA91"
   },
   "outputs": [],
   "source": [
    "num_examples_to_generate_4 = 512\n",
    "steps = 20\n",
    "total_data_length = data_len * steps\n",
    "total_noise_length = total_data_length - data_len + noise_len\n",
    "seed4 = tf.random.normal([num_examples_to_generate_4, total_noise_length, noise_channel])\n",
    "sample_data_4 = np.array([sequence_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "fD7nNLx_WA92",
    "outputId": "c3c70a6a-a7c2-479c-c02a-a4c2e00cf652"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "generated_image4 = recursive_simulator(generator, seed4, data_len, noise_len, steps)[:,:,0]\n",
    "generated_image4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "DRg2IieFWA92"
   },
   "outputs": [],
   "source": [
    "loss_obj_long = Loss(sample_data_4, real_mean=real_mean, real_std=real_std, max_lag=250, simple_loss=False, long_seq=True)\n",
    "loss_obj_long.compute_loss(generated_image4)\n",
    "# loss_obj_long.plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "F6qcf98hWA92",
    "outputId": "fb782461-cd81-4c71-d8d2-91c1488cba39"
   },
   "outputs": [],
   "source": [
    "loss_obj_long.loss_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9dx0OaAWA96"
   },
   "source": [
    "#### 300e training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Kc0KbAUSWA96",
    "outputId": "db2aeb7e-30b1-4726-ff37-28773af0228c"
   },
   "outputs": [],
   "source": [
    "train(data, BATCH_SIZE, generator, discriminator, generator_optimizer, discriminator_optimizer, test, 100, loss_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ow0UKzX9WA97"
   },
   "source": [
    "#### 300e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "0wVMQ4wpWA98"
   },
   "outputs": [],
   "source": [
    "num_examples_to_generate_4 = 512\n",
    "steps = 20\n",
    "total_data_length = data_len * steps\n",
    "total_noise_length = total_data_length - data_len + noise_len\n",
    "seed4 = tf.random.normal([num_examples_to_generate_4, total_noise_length, noise_channel])\n",
    "sample_data_4 = np.array([sequence_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "a2qSH38_WA98",
    "outputId": "42a9ea18-cba9-47cc-8057-adc9da02c458"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "generated_image4 = recursive_simulator(generator, seed4, data_len, noise_len, steps)[:,:,0]\n",
    "generated_image4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "BjUMhq2XWA99",
    "outputId": "f78f129c-ab99-4650-b165-fa17b98db74b"
   },
   "outputs": [],
   "source": [
    "loss_obj_long = Loss(sample_data_4, real_mean=real_mean, real_std=real_std, max_lag=250, simple_loss=False, long_seq=True)\n",
    "loss_obj_long.compute_loss(generated_image4)\n",
    "loss_obj_long.plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "9RQ_E0CMWA99",
    "outputId": "eb4b8602-57bf-4db5-be31-cf29c28fc0ba"
   },
   "outputs": [],
   "source": [
    "loss_obj_long.loss_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haQIQz6YWA-A"
   },
   "source": [
    "#### 400e training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3p-zaiaWWA-A",
    "outputId": "56ed2d75-531e-4af1-f9ba-b0b07245ad4d"
   },
   "outputs": [],
   "source": [
    "train(data, BATCH_SIZE, generator, discriminator, generator_optimizer, discriminator_optimizer, test, 100, loss_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piTwarO2WA-B"
   },
   "source": [
    "#### 400e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "OgsgzVq3WA-C"
   },
   "outputs": [],
   "source": [
    "num_examples_to_generate_4 = 512\n",
    "steps = 20\n",
    "total_data_length = data_len * steps\n",
    "total_noise_length = total_data_length - data_len + noise_len\n",
    "seed4 = tf.random.normal([num_examples_to_generate_4, total_noise_length, noise_channel])\n",
    "sample_data_4 = np.array([sequence_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "JWmNAIHSWA-D",
    "outputId": "e0ebfa46-e462-4d2a-b7ba-3a3cf8a2f888"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "generated_image4 = recursive_simulator(generator, seed4, data_len, noise_len, steps)[:,:,0]\n",
    "generated_image4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Ev8CPPsgWA-D",
    "outputId": "fb4a4b26-b156-44de-9aa6-611889a10a0b"
   },
   "outputs": [],
   "source": [
    "loss_obj_long = Loss(sample_data_4, real_mean=real_mean, real_std=real_std, max_lag=250, simple_loss=False, long_seq=True)\n",
    "loss_obj_long.compute_loss(generated_image4)\n",
    "loss_obj_long.plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "yPWlN17PWA-D",
    "outputId": "776ecc4c-11e0-4247-e657-1337400377c8"
   },
   "outputs": [],
   "source": [
    "loss_obj_long.loss_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qjgu2ilbiDdn"
   },
   "source": [
    "#### 500e training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "UTPlljOSiDdo",
    "outputId": "e48cabfd-92c0-42b0-c9ab-88b2713af55f"
   },
   "outputs": [],
   "source": [
    "train(data, BATCH_SIZE, generator, discriminator, generator_optimizer, discriminator_optimizer, test, 100, loss_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7yaozMDiDds"
   },
   "source": [
    "#### 500e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "FnBpbfpMiDds"
   },
   "outputs": [],
   "source": [
    "num_examples_to_generate_4 = 512\n",
    "steps = 20\n",
    "total_data_length = data_len * steps\n",
    "total_noise_length = total_data_length - data_len + noise_len\n",
    "seed4 = tf.random.normal([num_examples_to_generate_4, total_noise_length, noise_channel])\n",
    "sample_data_4 = np.array([sequence_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "vxGqn5MjiDdt",
    "outputId": "d61fb48b-10a1-4e90-afa8-4a275acaecbf"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "generated_image4 = recursive_simulator(generator, seed4, data_len, noise_len, steps)[:,:,0]\n",
    "generated_image4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "xMtT2qPHiDdu",
    "outputId": "26602039-14b8-455a-aede-a719763efb45"
   },
   "outputs": [],
   "source": [
    "loss_obj_long = Loss(sample_data_4, real_mean=real_mean, real_std=real_std, max_lag=250, simple_loss=False, long_seq=True)\n",
    "loss_obj_long.compute_loss(generated_image4)\n",
    "loss_obj_long.plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "saKMrLrriDdv",
    "outputId": "32f9f18d-6b0d-4af8-a207-886df2236765"
   },
   "outputs": [],
   "source": [
    "loss_obj_long.loss_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBizfsYp6drq"
   },
   "source": [
    "### TTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Q14IK36k6drr"
   },
   "outputs": [],
   "source": [
    "choice = 'transgan' \n",
    "opt_choice = 'wgan_gp'\n",
    "loss_choice = 'wgan_gp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "tLPJweF66drs"
   },
   "outputs": [],
   "source": [
    "gf_dim = 64\n",
    "g_attn = (2,2,4,False)\n",
    "df_dim = 64\n",
    "d_attn = (2,2,4,True)\n",
    "g_act = lambda: tf.keras.activations.gelu\n",
    "d_act = lambda: tf.keras.activations.gelu\n",
    "d_bn = True\n",
    "cumsum = True\n",
    "skip_con = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "wNsVAB456dru",
    "outputId": "7635b5d2-6c99-402c-f3be-5a44c1a627d5"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "data_len = 128\n",
    "data_channel = 1\n",
    "noise_len = 254\n",
    "noise_channel = 3\n",
    "\n",
    "data = np.array([standard_return[t:t + len(standard_return)-data_len] for t in range(0, data_len)]).T[:,:,np.newaxis]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Msg-_hO06drv"
   },
   "outputs": [],
   "source": [
    "def compare_test(model, epoch, test_input):\n",
    "  generated_image = generator(test_input, training=False)[:,:,0]\n",
    "  loss_obj_training.compute_loss(generated_image)\n",
    "  if (epoch%10 == 0) or (epoch == 1):\n",
    "    display.clear_output()\n",
    "    loss_obj_training.plot_loss()\n",
    "    loss_obj_training.plot_corr()\n",
    "    plt.show()\n",
    "\n",
    "num_examples_to_generate_2 = 512\n",
    "seed2 = tf.random.normal([num_examples_to_generate_2, noise_len, noise_channel])\n",
    "\n",
    "test = [[compare_test,seed2]]\n",
    "\n",
    "sample_data_2 = data[np.random.choice(data.shape[0], num_examples_to_generate_2, replace=False),:,0]\n",
    "loss_obj_training = Loss(sample_data_2, real_mean=real_mean, real_std=real_std, max_lag=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "y9mDF_ja6drw"
   },
   "outputs": [],
   "source": [
    "generator = make_generator_model(data_len, data_channel, noise_len, noise_channel, gf_dim=gf_dim, choice=choice, attn=g_attn, act=g_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "puP8wH6e6dry"
   },
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model(data_len, data_channel, choice, df_dim=df_dim, attn=d_attn, bn=d_bn, act=d_act, cumsum=cumsum, skip_con=skip_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "xamqVcDF6dr0"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = generator_optimizer_fun(opt_choice)\n",
    "discriminator_optimizer = discriminator_optimizer_fun(opt_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "pB1Ig97n6dr1"
   },
   "outputs": [],
   "source": [
    "G_loss = []; D_loss = [];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gC-mCxqb6dr2"
   },
   "source": [
    "#### 200e training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ipFSfFO26dr2",
    "outputId": "fc390b02-bbfd-4435-e155-fcdb032321e9"
   },
   "outputs": [],
   "source": [
    "train(data, BATCH_SIZE, generator, discriminator, generator_optimizer, discriminator_optimizer, test, 200, loss_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0I63zkfI6dr4"
   },
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "l-wn0UND6dr5"
   },
   "outputs": [],
   "source": [
    "num_examples_to_generate_4 = 512\n",
    "steps = 20\n",
    "total_data_length = data_len * steps\n",
    "total_noise_length = total_data_length - data_len + noise_len\n",
    "seed4 = tf.random.normal([num_examples_to_generate_4, total_noise_length, noise_channel])\n",
    "sample_data_4 = np.array([sequence_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "OGXnkjA16dr5",
    "outputId": "25eccd9e-515b-4f62-982a-cbd8eb0492e2"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "generated_image4 = recursive_simulator(generator, seed4, data_len, noise_len, steps)[:,:,0]\n",
    "generated_image4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "udypoIzU6dr6",
    "outputId": "bbf7a578-b103-4ace-fd53-b69613dad3c7"
   },
   "outputs": [],
   "source": [
    "loss_obj_long = Loss(sample_data_4, real_mean=real_mean, real_std=real_std, max_lag=250, simple_loss=False, long_seq=True)\n",
    "loss_obj_long.compute_loss(generated_image4)\n",
    "loss_obj_long.plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "mYZ43Sgi6dr6",
    "outputId": "d19ed35c-4e90-4be0-a4af-220a20470867"
   },
   "outputs": [],
   "source": [
    "loss_obj_long.loss_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A9gvoAc6dr-"
   },
   "source": [
    "#### 500e training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "vkGdD3IR6dr_",
    "outputId": "fac83870-6e6d-4042-cedc-22982d1a34c5"
   },
   "outputs": [],
   "source": [
    "train(data, BATCH_SIZE, generator, discriminator, generator_optimizer, discriminator_optimizer, test, 300, loss_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q284CuBu6dsA"
   },
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ded5AmM-6dsA"
   },
   "outputs": [],
   "source": [
    "num_examples_to_generate_4 = 512\n",
    "steps = 20\n",
    "total_data_length = data_len * steps\n",
    "total_noise_length = total_data_length - data_len + noise_len\n",
    "seed4 = tf.random.normal([num_examples_to_generate_4, total_noise_length, noise_channel])\n",
    "sample_data_4 = np.array([sequence_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "arusl6J56dsB",
    "outputId": "5625f173-70e3-44c0-86ff-f3540d064734"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "generated_image4 = recursive_simulator(generator, seed4, data_len, noise_len, steps)[:,:,0]\n",
    "generated_image4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "gjh7EofE6dsB",
    "outputId": "8b42e2c6-ba19-459c-9c04-35c7eb3143d6"
   },
   "outputs": [],
   "source": [
    "loss_obj_long = Loss(sample_data_4, real_mean=real_mean, real_std=real_std, max_lag=250, simple_loss=False, long_seq=True)\n",
    "loss_obj_long.compute_loss(generated_image4)\n",
    "loss_obj_long.plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "GNhjcuCZ6dsH",
    "outputId": "00c48f06-9ee8-4f55-9f80-c1a43aa0ae8a"
   },
   "outputs": [],
   "source": [
    "loss_obj_long.loss_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tm3sjA026dsJ"
   },
   "source": [
    "#### 800e training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "m4M-ggl_6dsK",
    "outputId": "ecfa31d4-f3d3-4ee7-d119-923d461c0642"
   },
   "outputs": [],
   "source": [
    "train(data, 2*BATCH_SIZE, generator, discriminator, generator_optimizer, discriminator_optimizer, test, 300, loss_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GT6gH6Fu6dsL"
   },
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Z5WPG_a26dsM"
   },
   "outputs": [],
   "source": [
    "num_examples_to_generate_4 = 512\n",
    "steps = 20\n",
    "total_data_length = data_len * steps\n",
    "total_noise_length = total_data_length - data_len + noise_len\n",
    "seed4 = tf.random.normal([num_examples_to_generate_4, total_noise_length, noise_channel])\n",
    "sample_data_4 = np.array([sequence_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "KfsDRMPM6dsM",
    "outputId": "96911f21-c409-4556-e086-3aa7dccd14cf"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "generated_image4 = recursive_simulator(generator, seed4, data_len, noise_len, steps)[:,:,0]\n",
    "generated_image4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Vrm7cXaS6dsN",
    "outputId": "5a1cf528-ecac-45fb-c11a-fd99a3cd4415"
   },
   "outputs": [],
   "source": [
    "loss_obj_long = Loss(sample_data_4, real_mean=real_mean, real_std=real_std, max_lag=250, simple_loss=False, long_seq=True)\n",
    "loss_obj_long.compute_loss(generated_image4)\n",
    "loss_obj_long.plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jXVNcR2i6dsN",
    "outputId": "1ce5a4ee-b4f0-4b45-c2c4-fa1a9299422f"
   },
   "outputs": [],
   "source": [
    "loss_obj_long.loss_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgOB4jXD6dsQ"
   },
   "source": [
    "#### 1100e training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDICNJAF6dsQ"
   },
   "outputs": [],
   "source": [
    "train(data, 4*BATCH_SIZE, generator, discriminator, generator_optimizer, discriminator_optimizer, test, 300, loss_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaKHtBTx6dsS"
   },
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzH7oax36dsT"
   },
   "outputs": [],
   "source": [
    "num_examples_to_generate_4 = 512\n",
    "steps = 20\n",
    "total_data_length = data_len * steps\n",
    "total_noise_length = total_data_length - data_len + noise_len\n",
    "seed4 = tf.random.normal([num_examples_to_generate_4, total_noise_length, noise_channel])\n",
    "sample_data_4 = np.array([sequence_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-xg-umo6dsT"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "generated_image4 = recursive_simulator(generator, seed4, data_len, noise_len, steps)[:,:,0]\n",
    "generated_image4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ppg3IXp6dsU"
   },
   "outputs": [],
   "source": [
    "loss_obj_long = Loss(sample_data_4, real_mean=real_mean, real_std=real_std, max_lag=250, simple_loss=False, long_seq=True)\n",
    "loss_obj_long.compute_loss(generated_image4)\n",
    "loss_obj_long.plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyOT5DCG6dsU"
   },
   "outputs": [],
   "source": [
    "loss_obj_long.loss_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kw2f74B16dsY"
   },
   "source": [
    "#### 1250e training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALtvk9PW6dsZ"
   },
   "outputs": [],
   "source": [
    "train(data, 4*BATCH_SIZE, generator, discriminator, generator_optimizer, discriminator_optimizer, test, 150, loss_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdt664rM6dsa"
   },
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_WO6mfx-6dsa"
   },
   "outputs": [],
   "source": [
    "num_examples_to_generate_4 = 512\n",
    "steps = 20\n",
    "total_data_length = data_len * steps\n",
    "total_noise_length = total_data_length - data_len + noise_len\n",
    "seed4 = tf.random.normal([num_examples_to_generate_4, total_noise_length, noise_channel])\n",
    "sample_data_4 = np.array([sequence_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxxia0Zh6dsb"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "generated_image4 = recursive_simulator(generator, seed4, data_len, noise_len, steps)[:,:,0]\n",
    "generated_image4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9naQQ56E6dsc"
   },
   "outputs": [],
   "source": [
    "loss_obj_long = Loss(sample_data_4, real_mean=real_mean, real_std=real_std, max_lag=250, simple_loss=False, long_seq=True)\n",
    "loss_obj_long.compute_loss(generated_image4)\n",
    "loss_obj_long.plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4m-9Mnp_6dsc"
   },
   "outputs": [],
   "source": [
    "loss_obj_long.loss_summary"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GAN_time_series_shared.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "VaccineMis",
   "language": "python",
   "name": "vaccinemis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
